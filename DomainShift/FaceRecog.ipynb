{"cells":[{"cell_type":"markdown","metadata":{"id":"w37l-2cm9DxH"},"source":["\n","# Seminararbeit\n","\n","##  Das verschlüsselte im Offensichtlichen: Emotionen\n"]},{"cell_type":"markdown","metadata":{"id":"_KLeDYLp9DxL"},"source":["_______________________________________________________________________________________________________________________________________________________\n","\n","### Emotion Facial Recognition of Drawn Characters: Transfer Learning Task\n","\n","_______________________________________________________________________________________________________________________________________________________"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VafVvR4m9DxM","executionInfo":{"status":"ok","timestamp":1652121037238,"user_tz":-120,"elapsed":2152,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}},"outputId":"31ff57f0-8402-4138-f6d8-0c2134158657"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","path_folder = \"/content/drive/MyDrive/Seminararbeit/Trainingsbilder\""]},{"cell_type":"markdown","metadata":{"id":"h5FlEzVk9DxM"},"source":["## Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rmmXfNy79DxN","executionInfo":{"status":"ok","timestamp":1652121041095,"user_tz":-120,"elapsed":3858,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import torch\n","import torchvision\n","from torchvision import transforms as T\n","\n","from scipy import ndimage\n","\n","import cv2 #for image processing\n","# import easygui #to open the filebox\n","import numpy as np #to store image\n","# import imageio #to read image stored at particular path\n","import sys\n","import matplotlib.pyplot as plt\n","import os\n","import tkinter as tk\n","from tkinter import filedialog\n","from tkinter import *\n","from PIL import ImageTk, Image\n","\n","from tqdm import tqdm\n","import collections"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gzq05qY-9DxO","executionInfo":{"status":"ok","timestamp":1652121041095,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}},"outputId":"3f849949-975c-48be-8d4e-0ae38d7110c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["anger\n","contempt\n","disgust\n","fear\n","happines\n","neutrality\n","sadness\n","surprise\n"]}],"source":["targets = [\"anger\",\"contempt\",\"disgust\",\"fear\",\"happiness\",\"neutrality\",\"sadness\",\"surprise\"]\n","\n","# A vector representation of the target for training later\n","tar_name2vec= {\n","    \"anger\"     : torch.tensor([1,0,0,0,0,0,0,0], dtype=torch.float32) ,\n","    \"contempt\"  : torch.tensor([0,1,0,0,0,0,0,0], dtype=torch.float32) ,\n","    \"disgust\"   : torch.tensor([0,0,1,0,0,0,0,0], dtype=torch.float32) ,\n","    \"fear\"      : torch.tensor([0,0,0,1,0,0,0,0], dtype=torch.float32) ,\n","    \"happiness\" : torch.tensor([0,0,0,0,1,0,0,0], dtype=torch.float32) ,\n","    \"neutrality\": torch.tensor([0,0,0,0,0,1,0,0], dtype=torch.float32) ,\n","    \"sadness\"   : torch.tensor([0,0,0,0,0,0,1,0], dtype=torch.float32) ,\n","    \"surprise\"  : torch.tensor([0,0,0,0,0,0,0,1], dtype=torch.float32) \n","}\n","\n","def tar_vec2name(vec):\n","    if vec[0] == 1: \n","        return \"anger\" \n","\n","    elif vec[1] == 1:\n","        return \"contempt\"\n","    \n","    elif vec[2] == 1:\n","        return \"disgust\"\n","\n","    elif vec[3] == 1:\n","        return \"fear\"\n","\n","    elif vec[4] == 1:\n","        return \"happines\"\n","\n","    elif vec[5] == 1:\n","        return \"neutrality\"\n","\n","    elif vec[6] == 1:\n","        return \"sadness\"\n","\n","    elif vec[7] == 1:\n","        return \"surprise\"\n","\n","for i in targets:\n","    print(tar_vec2name(tar_name2vec[i]))\n"]},{"cell_type":"markdown","metadata":{"id":"7GTxO52I9DxP"},"source":["________________________________"]},{"cell_type":"markdown","metadata":{"id":"GZh1WQ4e9DxQ"},"source":["## Hilfsfunktionen"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hUcHHDnl9DxQ","executionInfo":{"status":"ok","timestamp":1652121041096,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["# füge RGB hinzu\n","# von (y, x) zu (y, x, c)\n","def add_RGBLayers(im):\n","    if len(im.shape) == 3:\n","        return torch.tensor(im)\n","    elif len(im.shape) == 2:\n","        return torch.tensor(np.stack((im, im, im), 2))\n","    \n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_BYxGSaR9DxR","executionInfo":{"status":"ok","timestamp":1652121041096,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def calc_loss(network, loader):\n","    network.eval()\n","    loss = []\n","    g = 0\n","    crit = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n","    for b_idx, (dat, tar) in enumerate(loader):\n","        loss.append(crit(network(dat), tar).item())\n","        g += len(tar)\n","    \n","    return np.sum(loss)/g"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"DO3t-IFH9DxS","executionInfo":{"status":"ok","timestamp":1652121041096,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def max_Vektor(vec):\n","    x = torch.max(vec)\n","\n","    idx = (vec == x).nonzero().flatten()\n","\n","    t = torch.zeros_like(vec)\n","    t[idx] = 1\n","    return t"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"dbsHE6iC9DxS","executionInfo":{"status":"ok","timestamp":1652121041097,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}},"outputId":"9fb3499d-64d4-418a-fe08-016324dface5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'happines'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["tar_vec2name(max_Vektor(torch.tensor([0.1473, 0.0928, 0.1254, 0.1106, 0.2664, 0.1633, 0.0834, 0.0107])))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qA77yHXV9DxT","executionInfo":{"status":"ok","timestamp":1652121041097,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def get_different_classes(dict):\n","    d = {}\n","    \n","    for (dat, tar) in dict:\n","        if tar not in d:\n","            d[tar] = 1\n","        else:\n","            d[tar] += 1\n","\n","    return d"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"V1wwl4BS9DxT","executionInfo":{"status":"ok","timestamp":1652121041097,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def my_train_validation_split(dataset, test_train_split=0.8, balanced_split=True, shuffle=True, trans=None):\n","    \n","    #TODO balanced split\n","\n","    dataset_size = len(dataset)\n","    indices = list(range(dataset_size))\n","\n","    dict = {}\n","    for i in range(dataset_size):\n","        dict[i] = dataset.__getitem__(i)\n","        \n","\n","    # if balanced_split:\n","    #     dclasses = get_different_classes(dict)\n","\n","    trainset, testset = take_sample(dict, indices, test_train_split, shuffle)\n","            \n","    return myFaceImageDataset(trainset, transform=trans), myFaceImageDataset(testset)\n","\n","    #TODO balanced split\n","        "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hhlLcJ_P9DxT","executionInfo":{"status":"ok","timestamp":1652121041097,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def take_sample(dict, idx, split,shuffle=False):\n","    l = len(dict)\n","    if shuffle:\n","        np.random.shuffle(idx)\n","\n","    train = idx[:int(split * l)]\n","    test = idx[int(split * l):]\n","\n","    dtrain = {}\n","    dtest = {}\n","    counter = 0\n","    for i in train:\n","        dtrain[counter] = dict[i]\n","        counter+=1\n","    counter = 0\n","    for i in test:\n","        dtest[counter] = dict[i]\n","        counter+=1\n","    counter = 0\n","\n","    return dtrain, dtest\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Cy8TwRAj9DxU","executionInfo":{"status":"ok","timestamp":1652121041098,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["#return a certain number of file paths as a set; shuffled or not shuffled\n","def get_filenames(path_folder, number = 1000, shuffle = True):\n","    names = {}\n","    idx = 0\n","    for filename in os.listdir(path_folder):\n","\n","            f = os.path.join(path_folder,filename)\n","            if os.path.isfile(f):\n","                names[idx] = f\n","                idx += 1\n","    \n","    l = len(names)\n","    idx_vec = list(range(l))\n","    if shuffle:\n","        np.random.shuffle(idx_vec)\n","    \n","    if number > l: #Damit sichergestellt ist das nicht out of range gesliced werden kann\n","        number = l\n","    idx4Files_notUsed = idx_vec[number:]\n","\n","    for i in idx4Files_notUsed:\n","        names.pop(i)\n","\n","    return set(names.values())\n","                "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"XBPq3giJ9DxU","executionInfo":{"status":"ok","timestamp":1652121041098,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def load_balanced_data(path_folder, max_number_per_class = 1000, shuffle = True, add_RGB = True):\n","    data = {} \n","    count = 0\n","\n","    for t in (targets):    \n","        p = path_folder + \"\\\\\" + t\n","        # print(t)\n","        files2get = get_filenames(p, max_number_per_class, shuffle)\n","        for n in tqdm(files2get):\n","            # the sample is save on the 0. position and the target on the 1. position\n","            x = torch.tensor(plt.imread(n), dtype=torch.float32)\n","\n","            if add_RGB: \n","                x = add_RGBLayers(x)\n","\n","            data[count] = (x, tar_name2vec[t])\n","\n","            count += 1\n","\n","    return myFaceImageDataset(data, True)"]},{"cell_type":"markdown","metadata":{"id":"m50e_p-u9DxU"},"source":["# Image Augmentation\n","\n","Das Baselinemodell overfitter stark und die unterschiedlichen Klassen sind unbalanciert. Dies beischlusst die Vorhersage sehr stark, da z.B. für die Klasse \"happiness\" viel mehr Updates durchgeführt werden. Außerdem würde ein größerer Datensatz einen regularisierenden Effekt haben.\\\n","\n","[Pytorch: Augmentation Overview](https://datamahadev.com/performing-image-augmentation-using-pytorch/)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3XV-Bkx69DxV","executionInfo":{"status":"ok","timestamp":1652121041098,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["# wird nicht gebraucht, da die Augmentations auch im Dataset selbst erstellt werden könne, ohne alle vorher schon abgespeichert zu haben\n","def augmentation_pipeline_forOneImage(original_ima):\n","    original_ima = original_ima.transpose(-1,0)\n","    \n","\n","    augemntation_combis = [original_ima]\n","\n","    #all augmentations\n","    center_crop = torchvision.transforms.RandomCrop(size=(200,200), fill=0, padding_mode=\"constant\")\n","    horizontal_flip = torchvision.transforms.RandomVerticalFlip(p=1)\n","    random_rotation = torchvision.transforms.RandomRotation(degrees = 45)\n","    random_erase = torchvision.transforms.RandomErasing(p=1)\n","\n","    added_augments = []\n","    for i in augemntation_combis:\n","        added_augments.append(center_crop(i)) #cv2.resize(img, (new_w, new_h))\n","    augemntation_combis.extend(added_augments)\n","    \n","    added_augments = []\n","    for i in augemntation_combis:\n","        added_augments.append(horizontal_flip(i))\n","    augemntation_combis.extend(added_augments)\n","\n","    added_augments = []\n","    for i in augemntation_combis:\n","        added_augments.append(random_rotation(i))\n","    augemntation_combis.extend(added_augments)\n","\n","    added_augments = []\n","    for i in augemntation_combis:\n","        added_augments.append(random_erase(i))\n","    augemntation_combis.extend(added_augments)\n","\n","    #lighting the Image up a bit\n","    added_augments = []\n","    for i in augemntation_combis:\n","        added_augments.append(np.clip(i*1.2, 0, 1))\n","    augemntation_combis.extend(added_augments)\n","\n","    #making the Image a bit darker\n","    added_augments = []\n","    for i in augemntation_combis:\n","        added_augments.append(np.clip(i*0.8, 0, 1))\n","    augemntation_combis.extend(added_augments)\n","    \n","    for i in range(len(augemntation_combis)):\n","        augemntation_combis[i] = augemntation_combis[i].transpose(-1,0)\n","\n","    # damit die Dimensionalitäten bei allen Bilder gleich sind\n","    # Gab Problem beim Trainig, da manche Bilder 200x200x3 groß waren und andere 224x224x3\n","    for i in range(len(augemntation_combis)):  \n","        augemntation_combis[i] = augemntation_combis[i].numpy() * 255 # or any coefficient\n","        augemntation_combis[i] = augemntation_combis[i].astype(np.uint8)\n","        augemntation_combis[i] = cv2.resize(augemntation_combis[i], (224,224))\n","\n","\n","    return augemntation_combis\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Rb5hDoeT9DxV","executionInfo":{"status":"ok","timestamp":1652121041098,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def augmentation_pipeline_forFolder(origin_path_folder, destination_path_folder, add_RGB):\n","    #Loading the data into a the Notebook\n","\n","    for t in targets:    \n","        p = origin_path_folder + \"\\\\\" + t\n","        #print(t)\n","        for filename in os.listdir(p):\n","\n","            f = os.path.join(p,filename)\n","            if os.path.isfile(f):\n","\n","                # the sample is save on the 0. position and the target on the 1. position\n","                x = torch.tensor(plt.imread(f), dtype=torch.float32)\n","\n","                #print(count)\n","                #print(filename)\n","                if add_RGB: \n","                    x = add_RGBLayers(x)\n","\n","                aug = augmentation_pipeline_forOneImage(x)\n","               \n","                path = f.replace(origin_path_folder, destination_path_folder)\n","\n","                count = 0\n","                for i in aug:\n","                    \n","                    n = path.split(\"\\\\\")\n","                    n = n[-1].rsplit(\".\",1)\n","                    newName= n[0] +str(count)\n","                    extension=os.path.splitext(f)[1]\n","                    path1 = os.path.dirname(path)\n","                    path2 = os.path.join(path1, newName + extension)\n","            \n","                    cv2.imwrite(path2, i)\n","\n","                    count += 1\n","                      "]},{"cell_type":"markdown","metadata":{"id":"GbGBZE9U9DxW"},"source":["### Cartoonify Funktion mit OpenCV\n","\n","Source: https://data-flair.training/blogs/cartoonify-image-opencv-python/"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"VSFlOCLB9DxW","executionInfo":{"status":"ok","timestamp":1652121041560,"user_tz":-120,"elapsed":469,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def cartoonify(ImagePath):\n","    #read the image\n","    originalmage = cv2.imread(ImagePath)\n","    originalmage = cv2.cvtColor(originalmage, cv2.COLOR_BGR2RGB)\n","    #print(image)  # image is stored in form of numbers\n","    # confirm that image is chosen\n","    if originalmage is None:\n","        print(\"Can not find any image. Choose appropriate file\")\n","        sys.exit()\n","\n","    y, x, c = originalmage.shape\n","\n","    ReSized1 = cv2.resize(originalmage, (x, y))\n","    #plt.imshow(ReSized1, cmap='gray')\n","\n","    #converting an image to grayscale\n","    grayScaleImage = cv2.cvtColor(originalmage, cv2.COLOR_BGR2GRAY)\n","    ReSized2 = cv2.resize(grayScaleImage, (x, y))\n","    #plt.imshow(ReSized2, cmap='gray')\n","\n","    #applying median blur to smoothen an image\n","    smoothGrayScale = cv2.medianBlur(grayScaleImage, 5)\n","    ReSized3 = cv2.resize(smoothGrayScale, (x, y))\n","    #plt.imshow(ReSized3, cmap='gray')\n","\n","    #retrieving the edges for cartoon effect\n","    #by using thresholding technique\n","    getEdge = cv2.adaptiveThreshold(smoothGrayScale, 255, \n","    cv2.ADAPTIVE_THRESH_MEAN_C, \n","    cv2.THRESH_BINARY, 9, 9)\n","    ReSized4 = cv2.resize(getEdge, (x, y))\n","    #plt.imshow(ReSized4, cmap='gray')\n","\n","    #applying bilateral filter to remove noise \n","    #and keep edge sharp as required\n","    colorImage = cv2.bilateralFilter(originalmage, 9, 300, 300)\n","    ReSized5 = cv2.resize(colorImage, (x, y))\n","    #plt.imshow(ReSized5, cmap='gray')\n","\n","    #masking edged image with our \"BEAUTIFY\" image\n","    cartoonImage = cv2.bitwise_and(colorImage, colorImage, mask=getEdge)\n","    ReSized6 = cv2.resize(cartoonImage, (x, y))\n","    #plt.imshow(ReSized6, cmap='gray')\n","\n","    # Plotting the whole transition\n","    # images=[ReSized1, ReSized2, ReSized3, ReSized4, ReSized5, ReSized6]\n","    # fig, axes = plt.subplots(3,2, figsize=(8,8), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","    # for i, ax in enumerate(axes.flat):\n","    #     ax.imshow(images[i], cmap='gray')\n","    \n","    # plt.show()\n","\n","    return cartoonImage"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"mof0vQis9DxW","executionInfo":{"status":"ok","timestamp":1652121041561,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def save(Image, ImagePath):\n","\n","    #saving an image using imwrite()\n","    n = ImagePath.split(\"\\\\\")\n","    n = n[-1].rsplit(\".\",1)\n","    newName= n[0] +\"_cartoonified\"\n","    path1 = os.path.dirname(ImagePath)\n","    extension=os.path.splitext(ImagePath)[1]\n","    path = os.path.join(path1, newName + extension)\n","    cv2.imwrite(path, cv2.cvtColor(Image, cv2.COLOR_RGB2BGR))\n","\n","    #print(path)\n","\n","    # I = \"Image saved by name \" + newName +\" at \"+ path\n","    # tk.messagebox.showinfo(title=None, message=I)\n","   "]},{"cell_type":"markdown","metadata":{"id":"anCWwAw49DxX"},"source":["## Hilfsklassen\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pEEF9R6f9DxX","executionInfo":{"status":"ok","timestamp":1652121041561,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["class myFaceImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset_dict, transpose=False, transform = None):\n","        #das Übergeben Dict mit den Daten hat folgende Form:\n","        #{ key: index , value: list[ sample, label]}\n","        self.dataset_dict = dataset_dict\n","        self.transpose = transpose\n","\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset_dict)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        sample, target = self.dataset_dict[idx]\n","\n","        if self.transpose:\n","            #Transformiere das Sample, so dass es \n","            sample = sample.transpose(-1,0)\n","\n","        if self.transform != None:\n","            sample = self.transform(sample)\n","\n","        sample = torch.tensor(sample, dtype=torch.float32)\n","\n","        return (sample, target)"]},{"cell_type":"markdown","metadata":{"id":"Q3bPHtCX9DxX"},"source":["______________________________\n","## Pipeline für den Datensatz\n","\n","Der genutzte Datensatz ist auf Kaggle zu finden. \\\n","(https://www.kaggle.com/datasets/sudarshanvaidya/random-images-for-face-emotion-recognition?resource=download)."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"0hDQBmOD9DxX","executionInfo":{"status":"ok","timestamp":1652121041561,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def load_data(path_folder, add_RGB=False):\n","    #Loading the data into a the Notebook\n","    data = {}\n","    count = 0\n","\n","    for t in targets:    \n","        p = path_folder + \"/\" + t\n","        #print(t)\n","        for filename in os.listdir(p):\n","\n","            f = os.path.join(p,filename)\n","            if os.path.isfile(f):\n","\n","                # the sample is save on the 0. position and the target on the 1. position\n","                x = torch.tensor(plt.imread(f), dtype=float)\n","\n","                #print(count)\n","                #print(filename)\n","                if add_RGB: \n","                    x = add_RGBLayers(x)\n","\n","                data[count] = (x, tar_name2vec[t])\n","\n","                count += 1\n","\n","    return data"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0aZ9aYEj9DxY","executionInfo":{"status":"ok","timestamp":1652121041561,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def image_transformation_pipeline(path, transformation=\"CartCV2\"):\n","    \n","    \n","    #Files and directories in a specified path\n","\n","    #Hier wird über jede Datei iteriert, transformiert und in einen neuen Ordner gespeichert\n","    for filename in os.listdir(path):\n","\n","        f = os.path.join(path,filename)\n","        if os.path.isfile(f):\n","\n","            #nimmt den vorigen Path und passt in dann an\n","            fn = f.split(\"\\\\\")\n","\n","            #hier wird Entschieden welche Vorverarbeitung gewählt wird\n","            if transformation == \"CartCV2\":\n","\n","                #Entnimmt aus dem alten Pfad f\n","                test = cartoonify(f)\n","\n","                #print(f)\n","\n","                #Die Ordner existieren bereits\n","                fn[0] = fn[0] + \"_Cartoonified_OpenCV\"\n","                fn = \"\\\\\".join(fn)\n","\n","                #print(fn)\n","                #Steckt es in neuen Pfad fn\n","                save(test, fn)\n","\n","            # if transformation == \"DualGAN\":\n","\n","                # test = dualStyleGAN_transform(f)\n","\n","                # fn[0] = fn[0] + \"_Cartoonified_OpenCV\"\n","                # fn = \"\\\\\".join(fn)\n","                \n","                #print(fn)\n","                #Steckt es in neuen Pfad fn\n","                # save(test, fn)\n","\n","                # TODO\n","    "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"7QYfrNjD9DxY","executionInfo":{"status":"ok","timestamp":1652121041562,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["# for i in targets:\n","#     p = \"Trainingsbilder\\\\\" + i\n","#     image_transformation_pipeline(p)"]},{"cell_type":"markdown","metadata":{"id":"nS_f-cJv9DxY"},"source":["so apparently all images in the dataset have the same size, which makes things easier"]},{"cell_type":"markdown","metadata":{"id":"_YIO5JTc9DxY"},"source":["# Setting up the Model\n","\n","Dokumentation dazu: https://pytorch.org/vision/0.11/models.html"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"dpVdnhdi9DxY","executionInfo":{"status":"ok","timestamp":1652121041562,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["# Definition of the net structure\n","class Net(torch.nn.Module):\n","    def __init__(self, n_classes):\n","        super(Net, self).__init__()\n","        self.n_classes = n_classes\n","\n","        # Resnet Featureextractor LAden, ich werfe hier den großen LinearLayer am Ende von dem Resnet weg\n","        self.fe = torch.nn.Sequential(*(list(torchvision.models.resnet18(pretrained=True).children())[:-1]))\n","        # Die Featur Map die aus dem Resnet raus kommt hat eine Größe von batch_size x 512 x 1 x 1\n","        # -> Das bringen wir nacher in der forward Mehtode noch auf die richtige Dimension\n","\n","        # dein Lienar Layer muss aber 512 Eingänge haben\n","        self.fe2 = torch.nn.Sequential(torch.nn.Linear(512, 16),\n","                                        torch.nn.ReLU(),\n","                                        torch.nn.Dropout(),\n","                                        torch.nn.Linear(16, self.n_classes))\n","\n","        # Dropout sorgt für Regularisierung, gegen Overfitting\n","\n","    def forward(self, x):\n","        # den squeeze braucht man, weil die Feature Map eine Größe von batch_size x 512 x 1 x 1 hat,\n","        # die wird damit reduziert auf batch_size x 512\n","        y_pred = self.fe2(self.fe(x).squeeze())\n","\n","        return y_pred\n"]},{"cell_type":"markdown","metadata":{"id":"2Ixkgacd9DxZ"},"source":["# Modelltraining\n","\n","Interessante Dokumentationen:\\\n","[BCEWithLogitsLoss — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\\\n","[torch.nn.functional.one_hot — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html)\\\n","https://pytorch.org/docs/stable/data.html \\\n","https://pytorch.org/tutorials/beginner/data_loading_tutorial.html \\\n","https://pytorch.org/tutorials/beginner/basics/data_tutorial.html \\\n","[Data Augmentation](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html)\\\n","[Optimizing Model Parameters](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\\\n","[Modelle einfrieren](https://pytorch.org/docs/master/notes/autograd.html)\n","\n","Falls der DataLoader rummeckert müsste ich mir eine \"costum collate fnc\" erstellen (ins Dataset)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ru4xI2Q89DxZ","executionInfo":{"status":"ok","timestamp":1652121041562,"user_tz":-120,"elapsed":8,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["def train(net, train_l, test_l, optim):\n","    crit =  torch.nn.BCEWithLogitsLoss()\n","\n","\n","    bce_train = []\n","    bce_test = []\n","        \n","    \n","    \n","    for epoch in tqdm(range(0, epochen )):\n","        net.train()\n","        summ = []\n","        c = 0\n","        for b_idx, (dat, tar) in enumerate(train_l):\n","            # --- Training ----#\n","            optim.zero_grad()\n","            y_pred = net(dat)\n","            train_loss = crit(y_pred, tar)\n","            train_loss.backward()\n","            optim.step()\n","            \n","\n","            # ----- Evaluation metrics train ---- #\n","            summ.append(train_loss.item())\n","            c += 1\n","                \n","        bce_train.append(sum(summ)/c)\n","        \n","\n","        \n","        # ----- Test Prediction & Evaluation -------------#\n","        bce_test.append(calc_loss(net, test_l)) \n","        \n","    net.eval()\n","\n","    return net, bce_train, bce_test    "]},{"cell_type":"markdown","metadata":{"id":"R9kA-rZD9DxZ"},"source":["____________________________________________\n","### Vorverarbeitungsschritte"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"t4_QOZPg9DxZ","executionInfo":{"status":"ok","timestamp":1652121041563,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["# for i in targets:\n","#     p = \"Trainingsbilder\\\\\" + i\n","#     image_transformation_pipeline(p)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"y6yOAGMw9Dxa","executionInfo":{"status":"ok","timestamp":1652121041563,"user_tz":-120,"elapsed":9,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["# augmentation_pipeline_forFolder(\"J:\\\\Seminararbeit\\\\Trainingsbilder\",\n","#                                 \"J:\\\\Seminararbeit\\\\Trainingsbilder+Augmentations\", True)\n","\n","# brauch ich nicht mehr, da ich die Augmentation ins Dataset verlagere"]},{"cell_type":"markdown","metadata":{"id":"6fCYt0a-9Dxa"},"source":["__________________________"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"N7WM2Vwg9Dxa","executionInfo":{"status":"ok","timestamp":1652121041563,"user_tz":-120,"elapsed":7,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["learning_rate = 0.001\n","batch_size = 16\n","epochen = 2"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"cBO_NxPb9Dxa","executionInfo":{"status":"ok","timestamp":1652121041563,"user_tz":-120,"elapsed":7,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["aug_transformations = T.Compose([\n","    T.ColorJitter(brightness=(0.8,1.2)),\n","    T.RandomVerticalFlip(),\n","    T.RandomRotation(degrees=45),\n","    T.RandomErasing(),\n","    T.RandomCrop(size=(180,180), fill=0, padding_mode=\"constant\"),\n","    T.Resize(size=(224,224))\n","])"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4UXMRIp9Dxa","executionInfo":{"status":"ok","timestamp":1652121083650,"user_tz":-120,"elapsed":42094,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}},"outputId":"b7411a66-fffc-4d5a-9172-45749f9b57c0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]}],"source":["data = myFaceImageDataset(load_data(path_folder, add_RGB=True), transpose=True)\n","\n","data_train, data_val = my_train_validation_split(data, 0.9, shuffle=True, trans=aug_transformations)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"jSgKYf_J9Dxb","executionInfo":{"status":"ok","timestamp":1652121084528,"user_tz":-120,"elapsed":880,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["my_model = Net(8)\n","\n","#friert das pretrained ResNet ein\n","for param in my_model.fe.children():\n","    param.requires_grad_ = False"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"h1fbxc_b9Dxb","executionInfo":{"status":"ok","timestamp":1652121084528,"user_tz":-120,"elapsed":5,"user":{"displayName":"MoraiT 01","userId":"04021275033392705446"}}},"outputs":[],"source":["optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n","\n","# Note: Daten noch in Train und Test unterteilen und in ein TorchDataset transformieren\n","train_loader = torch.utils.data.DataLoader(\n","    data_train,\n","    batch_size = batch_size,\n","    shuffle = True\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    data_val,\n","    batch_size = len(data_val),\n","    shuffle = False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbgTSe6P9Dxb","outputId":"39d890fb-ebb7-4174-87e6-47c8c99e75ab"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]}],"source":["my_model, bce_train, bce_test = train(my_model, train_loader, val_loader, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1TxA8K59Dxb"},"outputs":[],"source":["fig, ax = plt.subplots(1,2, figsize=(18,6))\n","\n","ax[0].plot(bce_train,        color=\"navy\", label=\"Train Error\")\n","ax[0].set_xlabel(\"Anzahl an Updates\")\n","ax[0].set_ylabel(\"Trainingsfehler\")\n","\n","ax[1].plot(bce_test,  \"r--\", color=\"navy\", label=\"Test Error\" )\n","ax[1].set_xlabel(\"Anzahl an Epochen\")\n","ax[1].set_ylabel(\"Testfehler\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vtMVS7Wq9Dxb"},"source":["## Testgebiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJ-X8JWk9Dxb"},"outputs":[],"source":["# torch.save(my_model.state_dict(), \"Baselinemodell\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eR-Q4n0I9Dxc"},"outputs":[],"source":["# torch.save(data_train, \"Trainingsbilder/Train\")\n","# torch.save(data_val, \"Trainingsbilder/Validation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNrLSaZ59Dxc"},"outputs":[],"source":["loaded_model = Net(8)\n","loaded_model.load_state_dict(torch.load(\"Modell1/Baselinemodell\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRUQG5Pw9Dxc"},"outputs":[],"source":["d = data[0][0]\n","print(d.shape)\n","d = torch.unsqueeze(d, 0)\n","print(d.shape)\n","\n","# Net().forward(d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjCZ2zSd9Dxc"},"outputs":[],"source":["test = cv2.imread(\"Trainingsbilder/happiness/7647334524_7fe96ff1f0_n_face.png\")\n","print(test.shape)\n","plt.imshow(test, cmap=\"gray\")\n","# Alle Bilder sind nur Graubilder\n","# habe keine 3. Dimesion -> könnten also falsch interpretiert werden.\n","# D.h. als zusätzlichen Vorverabeitungsschritt sollte ggf. RGB hinzugefügt werden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omkeblEE9Dxd"},"outputs":[],"source":["#test= add_RGBLayers(test)\n","plt.imshow(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDcjYn849Dxd"},"outputs":[],"source":["lowpass = ndimage.gaussian_filter(test, 3)\n","gauss_highpass = (test - lowpass + np.uint8(np.ones_like(test)* 1))\n","\n","print(gauss_highpass.shape)\n","\n","plt.imshow(add_RGBLayers(gauss_highpass))\n","print(np.mean(gauss_highpass))\n","print(np.mean(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK5_xeLV9Dxd"},"outputs":[],"source":["def black_white_transform(v, mean=100):\n","    if v < mean:\n","        return 0\n","    else:\n","        return 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2HvkEZV9Dxd"},"outputs":[],"source":["tfunc = np.vectorize(black_white_transform)\n","test = tfunc(test)\n","plt.imshow(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02QwTZ8I9Dxd"},"outputs":[],"source":["cim = cartoonify(\"Trainingsbilder/happiness/7647334524_7fe96ff1f0_n_face.png\")\n","\n","cim.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nIwWMT29Dxe"},"outputs":[],"source":["plt.imshow(cim)\n","cim = torch.tensor(cim, dtype=torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5XJEedC_9Dxe"},"outputs":[],"source":["(cim.transpose(-1,0)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmY9EPCC9Dxe"},"outputs":[],"source":["y = loaded_model(cim.transpose(-1,0).unsqueeze(dim=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSUjwbs39Dxe"},"outputs":[],"source":["print(torch.softmax(y, dim=0, dtype=torch.float32))\n","print(torch.max(torch.softmax(y, dim=0, dtype=torch.float32)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bXo71eQ9Dxe"},"outputs":[],"source":["cim2 = plt.imread(\"Trainingsbilder_Cartoonified_OpenCV/contempt/images - 2020-11-06T181758.699_face_cartoonified.png\")\n","plt.imshow(cim2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ua5QeaN9Dxf"},"outputs":[],"source":["cim2 = torch.tensor(cim2, dtype = torch.float32)\n","(cim2.transpose(-1,0)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrdUd0G49Dxf"},"outputs":[],"source":["y = loaded_model(cim2.transpose(-1,0).unsqueeze(dim=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ss-0uCE9Dxf"},"outputs":[],"source":["print(torch.softmax(y, dim=0, dtype=torch.float32))\n","print(torch.max(torch.softmax(y, dim=0, dtype=torch.float32)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F82jyTb89Dxf"},"outputs":[],"source":["drawn = torch.tensor(plt.imread(\"Testbilder/yesyesyes w.png\"), dtype=torch.float32)\n","#plt.imshow(drawn)\n","drawn = drawn[:,:,:3]\n","print(drawn.shape)\n","plt.imshow(drawn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pF5TPlO9Dxf"},"outputs":[],"source":["drawn_predict = loaded_model(drawn.transpose(-1,0).unsqueeze(dim=0))\n","print(torch.softmax(drawn_predict, dim=0, dtype=torch.float32))\n","print(torch.max(torch.softmax(drawn_predict, dim=0, dtype=torch.float32)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAs5L4UR9Dxg"},"outputs":[],"source":["drawn = torch.tensor(plt.imread(\"Testbilder/2TManager.png\"), dtype=torch.float32)\n","#plt.imshow(drawn)\n","drawn = drawn[:,:,:3]\n","print(drawn.shape)\n","plt.imshow(drawn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yU5G5S69Dxg"},"outputs":[],"source":["drawn_predict = loaded_model(drawn.transpose(-1,0).unsqueeze(dim=0))\n","print(torch.softmax(drawn_predict, dim=0, dtype=torch.float32))\n","print(torch.max(torch.softmax(drawn_predict, dim=0, dtype=torch.float32)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCcmRapS9Dxg"},"outputs":[],"source":["#save(cim, \"Trainingsbilder_Cartoonified_OpenCV/happiness/7647334524_7fe96ff1f0_n_face.png\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDxpZ--o9Dxg"},"outputs":[],"source":["a = \"Trainingsbilder_Cartoonified_OpenCV/happiness/7647334524_7fe96ff1f0_n_face.png\".split(\"/\")\n","a = a[-1].split(\".\")\n","a = a[0] +\"_cartoonified.\"+ a[1]\n","a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P31ix2T89Dxg"},"outputs":[],"source":["originalmage = cv2.imread(\"Trainingsbilder\\\\happiness\\\\7647334524_7fe96ff1f0_n_face.png\")\n","plt.imshow(originalmage)\n","print(originalmage.shape)\n","x, y, c = originalmage.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lc7MxV_m9Dxg"},"outputs":[],"source":["path_of_the_directory= \"Trainingsbilder\\happiness\"\n","print(\"Files and directories in a specified path:\")\n","for filename in os.listdir(path_of_the_directory):\n","    f = os.path.join(path_of_the_directory,filename)\n","    if os.path.isfile(f):\n","        print(f)\n","        fn = f.split(\"\\\\\")\n","        fn[0] = fn[0] + \"_Cartoonified_OpenCV\"\n","        fn = \"/\".join(fn)\n","        print(fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9ZR8oZc9Dxh"},"outputs":[],"source":["# teste alle Augmetations an Lena aus\n","ttt = torch.tensor(plt.imread(\"images - 2020-11-06T002957.309_face.png\"))\n","ttt = add_RGBLayers(ttt)\n","plt.imshow(ttt)\n","\n","a = augmentation_pipeline_forOneImage(ttt)\n","\n","print(len(a))\n","for i in a:\n","    plt.imshow(i, cmap=\"gray\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OND6R4F89Dxh"},"source":[""]}],"metadata":{"interpreter":{"hash":"e65133fff93ab36b53ddb68612dd6a95a21053ff6cb8524adc7f46061287cbaa"},"kernelspec":{"display_name":"Python 3.9.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"orig_nbformat":4,"colab":{"name":"FaceRecog.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}